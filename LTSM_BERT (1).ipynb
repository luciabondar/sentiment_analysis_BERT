{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LTSM_BERT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "drive.mount('/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6q-dyijWAfF1",
        "outputId": "575e076c-9c20-4e1b-e768-e927d31cc8a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /drive; to attempt to forcibly remount, call drive.mount(\"/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.chdir(\"/drive/MyDrive/inflation_reports\") \n",
        "os.getcwd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YxDFa8x-Af1i",
        "outputId": "0b2a2c29-bc4f-4f58-f1a7-2d9edacf1387"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/drive/MyDrive/inflation_reports'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 192
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/drive/MyDrive/inflation_reports/dataset/data.csv\"\n",
        "df = pd.read_csv(path, encoding='latin-1')"
      ],
      "metadata": {
        "id": "cxLVtX_iAyaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def to_sentiment(rating):\n",
        "\n",
        "#   if rating == 'negative':\n",
        "#     return 0\n",
        "#   elif rating == 'neutral':\n",
        "#     return 1\n",
        "#   elif rating == 'positive':\n",
        "#     return 2\n",
        "\n",
        "# df['score'] = df.sentiment.apply(to_sentiment)"
      ],
      "metadata": {
        "id": "itbFsXdO2bc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.6.0 torchvision==0.7.0 torchtext==0.7.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sxrv9TL8_GRt",
        "outputId": "30d0ce9f-59bd-45bb-db2c-3fa5e21138d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch==1.6.0 in /usr/local/lib/python3.7/dist-packages (1.6.0)\n",
            "Requirement already satisfied: torchvision==0.7.0 in /usr/local/lib/python3.7/dist-packages (0.7.0)\n",
            "Requirement already satisfied: torchtext==0.7.0 in /usr/local/lib/python3.7/dist-packages (0.7.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0) (1.21.6)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.7.0) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.7.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.7.0) (4.64.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from torchtext==0.7.0) (0.1.96)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.7.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.7.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.7.0) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.7.0) (1.25.11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "WFCWCSBI_M52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check whether cuda is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXaRljrn_M-W",
        "outputId": "68d33d2f-a258-4202-f518-8ef2c117266b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32h6CMGG_VPN",
        "outputId": "f6449fa6-b3bc-471c-c556-42b663fb43a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "init_token = tokenizer.cls_token\n",
        "eos_token = tokenizer.sep_token\n",
        "pad_token = tokenizer.pad_token\n",
        "unk_token = tokenizer.unk_token\n",
        "\n",
        "print(init_token, eos_token, pad_token, unk_token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoSJOxJC_aoF",
        "outputId": "5ebc86a7-851e-4208-d27b-6860601f3d8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] [SEP] [PAD] [UNK]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "init_token_idx = tokenizer.cls_token_id\n",
        "eos_token_idx = tokenizer.sep_token_id\n",
        "pad_token_idx = tokenizer.pad_token_id\n",
        "unk_token_idx = tokenizer.unk_token_id\n",
        "\n",
        "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYEihtQBAD-z",
        "outputId": "037a8cc5-fc53-453f-c301-e12220521012"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "101 102 0 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
        "\n",
        "print(max_input_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmnjgDXpAGPO",
        "outputId": "75fc943e-f4a2-4c71-fc2f-d6806174c941"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_cut(sentence):\n",
        "    tokens = tokenizer.tokenize(sentence) \n",
        "    tokens = tokens[:max_input_length-2]\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "spVXQ6-EAIiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6y0UY97ARmr",
        "outputId": "3a2d3973-9f9a-45b7-8296-398c85ba7db1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.7/dist-packages (0.7.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.21.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext) (2.23.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from torchtext) (0.1.96)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (1.25.11)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch->torchtext) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext import data"
      ],
      "metadata": {
        "id": "aO041l-KALlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT = data.Field(batch_first = True,\n",
        "                  use_vocab = False,\n",
        "                  tokenize = tokenize_and_cut,\n",
        "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
        "                  init_token = init_token_idx,\n",
        "                  eos_token = eos_token_idx,\n",
        "                  pad_token = pad_token_idx,\n",
        "                  unk_token = unk_token_idx)\n",
        "\n",
        "LABEL = data.LabelField(batch_first = True,dtype = torch.float)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNftHERIAOch",
        "outputId": "e128cc73-9a2b-4abc-d3eb-4f98b2a5f74a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/field.py:150: UserWarning: LabelField class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "for col in df.columns:\n",
        "    print(col)\n",
        "\n",
        "train_data, valid_data = train_test_split(df, test_size=0.1, random_state=RANDOM_SEED)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPxaQOQvBhbt",
        "outputId": "e85e2d5c-573b-4fcd-db55-6cc22d565742"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label\n",
            "text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "import datasets\n",
        "from datasets import Dataset, ClassLabel\n",
        "dataset = Dataset.from_pandas(df)\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ospYAoXhCTGn",
        "outputId": "fcc4855d-294c-4d99-e7a3-e4a056ac7cff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.2.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n",
            "Requirement already satisfied: dill<0.3.5 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.4)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.7.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.5.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.7.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['label', 'text'],\n",
              "    num_rows: 4846\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 268
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "df.rename(columns = {'sentence':'text', 'sentiment':'label'}, inplace = True)\n",
        "\n",
        "df_train, df_test = train_test_split(df, test_size=0.4, random_state=RANDOM_SEED)"
      ],
      "metadata": {
        "id": "31_D6au0Cj_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_train.iloc[:, ::-1]\n",
        "df_test = df_test.iloc[:, ::-1]"
      ],
      "metadata": {
        "id": "sRvXcseDSlL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "q-4ayratSUhm",
        "outputId": "2e5c7f32-2f03-4c20-893e-a3808908f7c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   text     label\n",
              "3207  The company was supposed to deliver machinery ...   neutral\n",
              "1684  UNC Charlotte would also deploy SSH Tectia Con...   neutral\n",
              "1044  In 2009 , Lee & Man had a combined annual prod...   neutral\n",
              "4145  `` That 's a very high figure on the European ...   neutral\n",
              "1538  In Finland , the corresponding service is Alma...   neutral\n",
              "...                                                 ...       ...\n",
              "550   Kalnapilio-Tauro Grupe ( Kalnapilis-Tauras Gro...  positive\n",
              "1424  The shares shall be repurchased through public...   neutral\n",
              "2939  Honkarakenne also decided yesterday to sell 88...   neutral\n",
              "2836  Blyk is launching first in the UK market in mi...   neutral\n",
              "3413  Therefore , Phase III of the research will not...   neutral\n",
              "\n",
              "[1939 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7d7bbd2a-e7be-4774-8d8a-619f56e620fa\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3207</th>\n",
              "      <td>The company was supposed to deliver machinery ...</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1684</th>\n",
              "      <td>UNC Charlotte would also deploy SSH Tectia Con...</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1044</th>\n",
              "      <td>In 2009 , Lee &amp; Man had a combined annual prod...</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4145</th>\n",
              "      <td>`` That 's a very high figure on the European ...</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1538</th>\n",
              "      <td>In Finland , the corresponding service is Alma...</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>550</th>\n",
              "      <td>Kalnapilio-Tauro Grupe ( Kalnapilis-Tauras Gro...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1424</th>\n",
              "      <td>The shares shall be repurchased through public...</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2939</th>\n",
              "      <td>Honkarakenne also decided yesterday to sell 88...</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2836</th>\n",
              "      <td>Blyk is launching first in the UK market in mi...</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3413</th>\n",
              "      <td>Therefore , Phase III of the research will not...</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1939 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7d7bbd2a-e7be-4774-8d8a-619f56e620fa')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7d7bbd2a-e7be-4774-8d8a-619f56e620fa button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7d7bbd2a-e7be-4774-8d8a-619f56e620fa');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 324
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.to_csv('/drive/MyDrive/inflation_reports/dataset/train.csv', sep=\"\\t\",index=False)"
      ],
      "metadata": {
        "id": "Bz-nqwK5JRmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.to_csv('/drive/MyDrive/inflation_reports/dataset/test.csv', sep=\"\\t\", index=False)"
      ],
      "metadata": {
        "id": "67uQ6wgkJuty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fields = [('text', TEXT), ('label', LABEL)]\n",
        "#loading custom dataset\n",
        "training_data=data.TabularDataset(path =\"/drive/MyDrive/inflation_reports/dataset/train.csv\",format = 'tsv',fields = fields,skip_header = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQTkyJrSJ0vp",
        "outputId": "e77cea58-cae8-4a7f-904a-ca2e5ecc107b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_data.Features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-MIQGo6ONV6",
        "outputId": "53168acf-d9d3-4b02-8e41-6a668cce71d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object Dataset.__getattr__ at 0x7ff5c0d7eb50>"
            ]
          },
          "metadata": {},
          "execution_count": 328
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, valid_data = training_data.split(split_ratio=0.8, random_state = random.seed(SEED))   #, random_state = random.seed(SEED)\n",
        "# train_data, valid_data = train_data.split(split_ratio=0.8, random_state=random.seed(SEED))"
      ],
      "metadata": {
        "id": "Qqv3gQoaJ9dO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_data))\n",
        "print(len(valid_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FglndfAYKA0-",
        "outputId": "9d72c3d7-3f79-45fa-b629-919984dbe038"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2326\n",
            "581\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "print(LABEL.vocab.itos)\n",
        "# No. of unique tokens in label\n",
        "print(\"Size of LABEL vocabulary:\", len(LABEL.vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKyqukMlCshq",
        "outputId": "cf4c5894-56d4-445e-eb09-788bdfd93f4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['neutral', 'positive', 'negative']\n",
            "Size of LABEL vocabulary: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 16\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "train_iterator, valid_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data), \n",
        "    batch_size = BATCH_SIZE, \n",
        "    sort_key=lambda x: len(x.text),\n",
        "\n",
        "    device = device)\n",
        "\n",
        "for i in valid_iterator:\n",
        "    print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtbQwuoLCycY",
        "outputId": "1231688f-e6e7-41ca-8aeb-d6ac490c898a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "\n",
            "[torchtext.data.batch.Batch of size 16]\n",
            "\t[.text]:[torch.cuda.LongTensor of size 16x11 (GPU 0)]\n",
            "\t[.label]:[torch.cuda.FloatTensor of size 16 (GPU 0)]\n",
            "\n",
            "[torchtext.data.batch.Batch of size 16]\n",
            "\t[.text]:[torch.cuda.LongTensor of size 16x13 (GPU 0)]\n",
            "\t[.label]:[torch.cuda.FloatTensor of size 16 (GPU 0)]\n",
            "\n",
            "[torchtext.data.batch.Batch of size 16]\n",
            "\t[.text]:[torch.cuda.LongTensor of size 16x15 (GPU 0)]\n",
            "\t[.label]:[torch.cuda.FloatTensor of size 16 (GPU 0)]\n",
            "\n",
            "[torchtext.data.batch.Batch of size 16]\n",
            "\t[.text]:[torch.cuda.LongTensor of size 16x16 (GPU 0)]\n",
            "\t[.label]:[torch.cuda.FloatTensor of size 16 (GPU 0)]\n",
            "\n",
            "[torchtext.data.batch.Batch of size 16]\n",
            "\t[.text]:[torch.cuda.LongTensor of size 16x17 (GPU 0)]\n",
            "\t[.label]:[torch.cuda.FloatTensor of size 16 (GPU 0)]\n",
            "\n",
            "[torchtext.data.batch.Batch of size 16]\n",
            "\t[.text]:[torch.cuda.LongTensor of size 16x18 (GPU 0)]\n",
            "\t[.label]:[torch.cuda.FloatTensor of size 16 (GPU 0)]\n",
            "\n",
            "[torchtext.data.batch.Batch of size 16]\n",
            "\t[.text]:[torch.cuda.LongTensor of size 16x18 (GPU 0)]\n",
            "\t[.label]:[torch.cuda.FloatTensor of size 16 (GPU 0)]\n",
            "\n",
            "[torchtext.data.batch.Batch of size 16]\n",
            "\t[.text]:[torch.cuda.LongTensor of size 16x19 (GPU 0)]\n",
            "\t[.label]:[torch.cuda.FloatTensor of size 16 (GPU 0)]\n",
            "\n",
            "[torchtext.data.batch.Batch of size 16]\n",
            "\t[.text]:[torch.cuda.LongTensor of size 16x20 (GPU 0)]\n",
            "\t[.label]:[torch.cuda.FloatTensor of size 16 (GPU 0)]\n",
            "\n",
            "[torchtext.data.batch.Batch of size 16]\n",
            "\t[.text]:[torch.cuda.LongTensor of size 16x21 (GPU 0)]\n",
            "\t[.label]:[torch.cuda.FloatTensor of size 16 (GPU 0)]\n",
            "\n",
            "[torchtext.data.batch.Batch of size 16]\n",
            "\t[.text]:[torch.cuda.LongTensor of size 16x22 (GPU 0)]\n",
            "\t[.label]:[torch.cuda.FloatTensor of size 16 (GPU 0)]\n",
            "\n",
            "[torchtext.data.batch.Batch of size 16]\n",
            "\t[.text]:[torch.cuda.LongTensor of size 16x23 (GPU 0)]\n",
            "\t[.label]:[torch.cuda.FloatTensor of size 16 (GPU 0)]\n",
            "\n",
            "[torchtext.data.batch.Batch of size 16]\n",
            "\t[.text]:[torch.cuda.LongTensor of size 16x24 (GPU 0)]\n",
            "\t[.label]:[torch.cuda.FloatTensor of size 16 (GPU 0)]\n",
            "\n",
            "[torchtext.data.batch.Batch of size 16]\n",
            "\t[.text]:[torch.cuda.LongTensor of size 16x25 (GPU 0)]\n",
            "\t[.label]:[torch.cuda.FloatTensor of size 16 (GPU 0)]\n",
            "\n",
            "[torchtext.data.batch.Batch of size 16]\n",
            "\t[.text]:[torch.cuda.LongTensor of size 16x26 (GPU 0)]\n",
            "\t[.label]:[torch.cuda.FloatTensor of size 16 (GPU 0)]\n",
            "\n",
            "[torchtext.data.batch.Batch of size 16]\n",
            "\t[.text]:[torch.cuda.LongTensor of size 16x27 (GPU 0)]\n",
            "\t[.label]:[torch.cuda.FloatTensor of size 16 (GPU 0)]\n",
            "\n",
            "[torchtext.data.batch.Batch of size 16]\n",
            "\t[.text]:[torch.cuda.LongTensor of size 16x27 (GPU 0)]\n",
            "\t[.label]:[torch.cuda.FloatTensor of size 16 (GPU 0)]\n",
            "\n",
            "[torchtext.data.batch.Batch of size 16]\n",
            "\t[.text]:[torch.cuda.LongTensor of size 16x28 (GPU 0)]\n",
            "\t[.label]:[torch.cuda.FloatTensor of size 16 (GPU 0)]\n",
            "\n",
            "[torchtext.data.batch.Batch of size 16]\n",
            "\t[.text]:[torch.cuda.LongTensor of size 16x28 (GPU 0)]\n",
            "\t[.label]:[torch.cuda.FloatTensor of size 16 (GPU 0)]\n",
            "\n",
            "[torchtext.data.batch.Batch of size 16]\n",
            "\t[.text]:[torch.cuda.LongTensor of size 16x29 (GPU 0)]\n",
            "\t[.label]:[torch.cuda.FloatTensor of size 16 (GPU 0)]\n",
            "\n",
            "[torchtext.data.batch.Batch of size 16]\n",
            "\t[.text]:[torch.cuda.LongTensor of size 16x30 (GPU 0)]\n",
            "\t[.label]:[torch.cuda.FloatTensor of size 16 (GPU 0)]\n",
            "\n",
            "[torchtext.data.batch.Batch of size 16]\n",
            "\t[.text]:[torch.cuda.LongTensor of size 16x31 (GPU 0)]\n",
            "\t[.label]:[torch.cuda.FloatTensor of size 16 (GPU 0)]\n",
            "\n",
            "[torchtext.data.batch.Batch of size 16]\n",
            "\t[.text]:[torch.cuda.LongTensor of size 16x32 (GPU 0)]\n",
            "\t[.label]:[torch.cuda.FloatTensor of size 16 (GPU 0)]\n",
            "\n",
            "[torchtext.data.batch.Batch of size 16]\n",
            "\t[.text]:[torch.cuda.LongTensor of size 16x33 (GPU 0)]\n",
            "\t[.label]:[torch.cuda.FloatTensor of size 16 (GPU 0)]\n",
            "\n",
            "[torchtext.data.batch.Batch of size 16]\n",
            "\t[.text]:[torch.cuda.LongTensor of size 16x35 (GPU 0)]\n",
            "\t[.label]:[torch.cuda.FloatTensor of size 16 (GPU 0)]\n",
            "\n",
            "[torchtext.data.batch.Batch of size 16]\n",
            "\t[.text]:[torch.cuda.LongTensor of size 16x36 (GPU 0)]\n",
            "\t[.label]:[torch.cuda.FloatTensor of size 16 (GPU 0)]\n",
            "\n",
            "[torchtext.data.batch.Batch of size 16]\n",
            "\t[.text]:[torch.cuda.LongTensor of size 16x38 (GPU 0)]\n",
            "\t[.label]:[torch.cuda.FloatTensor of size 16 (GPU 0)]\n",
            "\n",
            "[torchtext.data.batch.Batch of size 16]\n",
            "\t[.text]:[torch.cuda.LongTensor of size 16x39 (GPU 0)]\n",
            "\t[.label]:[torch.cuda.FloatTensor of size 16 (GPU 0)]\n",
            "\n",
            "[torchtext.data.batch.Batch of size 16]\n",
            "\t[.text]:[torch.cuda.LongTensor of size 16x41 (GPU 0)]\n",
            "\t[.label]:[torch.cuda.FloatTensor of size 16 (GPU 0)]\n",
            "\n",
            "[torchtext.data.batch.Batch of size 16]\n",
            "\t[.text]:[torch.cuda.LongTensor of size 16x43 (GPU 0)]\n",
            "\t[.label]:[torch.cuda.FloatTensor of size 16 (GPU 0)]\n",
            "\n",
            "[torchtext.data.batch.Batch of size 16]\n",
            "\t[.text]:[torch.cuda.LongTensor of size 16x45 (GPU 0)]\n",
            "\t[.label]:[torch.cuda.FloatTensor of size 16 (GPU 0)]\n",
            "\n",
            "[torchtext.data.batch.Batch of size 16]\n",
            "\t[.text]:[torch.cuda.LongTensor of size 16x47 (GPU 0)]\n",
            "\t[.label]:[torch.cuda.FloatTensor of size 16 (GPU 0)]\n",
            "\n",
            "[torchtext.data.batch.Batch of size 16]\n",
            "\t[.text]:[torch.cuda.LongTensor of size 16x48 (GPU 0)]\n",
            "\t[.label]:[torch.cuda.FloatTensor of size 16 (GPU 0)]\n",
            "\n",
            "[torchtext.data.batch.Batch of size 16]\n",
            "\t[.text]:[torch.cuda.LongTensor of size 16x53 (GPU 0)]\n",
            "\t[.label]:[torch.cuda.FloatTensor of size 16 (GPU 0)]\n",
            "\n",
            "[torchtext.data.batch.Batch of size 16]\n",
            "\t[.text]:[torch.cuda.LongTensor of size 16x58 (GPU 0)]\n",
            "\t[.label]:[torch.cuda.FloatTensor of size 16 (GPU 0)]\n",
            "\n",
            "[torchtext.data.batch.Batch of size 16]\n",
            "\t[.text]:[torch.cuda.LongTensor of size 16x69 (GPU 0)]\n",
            "\t[.label]:[torch.cuda.FloatTensor of size 16 (GPU 0)]\n",
            "\n",
            "[torchtext.data.batch.Batch of size 5]\n",
            "\t[.text]:[torch.cuda.LongTensor of size 5x150 (GPU 0)]\n",
            "\t[.label]:[torch.cuda.FloatTensor of size 5 (GPU 0)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "bert = BertModel.from_pretrained('bert-base-uncased') # it is important to use the same model and tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLqLYJPKC0L3",
        "outputId": "05102bbe-09b0-47df-a004-e7e25d0cf31c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class BERTLSTMSentiment(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_dim,\n",
        "                 output_dim,\n",
        "                 n_layers,\n",
        "                 bidirectional,\n",
        "                 dropout):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.bert = bert\n",
        "        \n",
        "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
        "        \n",
        "        self.rnn = nn.LSTM(embedding_dim,\n",
        "                          hidden_dim,\n",
        "                          num_layers = n_layers,\n",
        "                          bidirectional = bidirectional,\n",
        "                          batch_first = True,\n",
        "                          dropout = dropout)\n",
        "        \n",
        "        self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text):\n",
        "        \n",
        "        print('text:', text)\n",
        "        #text = [batch size, sent len]\n",
        "                \n",
        "        with torch.no_grad():\n",
        "            print('embedded dimensions', self.bert(text)[0] )\n",
        "            embedded = self.bert(text)[0][0]\n",
        "            print(self,bert(text)[0][0])\n",
        "\n",
        "        print('embedded:', embedded)    \n",
        "                \n",
        "        #embedded = [batch size, sent len, emb dim]\n",
        "        \n",
        "        _, hidden = self.rnn(embedded)\n",
        "        \n",
        "        #hidden = [n layers * n directions, batch size, emb dim]\n",
        "        \n",
        "        if self.rnn.bidirectional:\n",
        "            \n",
        "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 3))\n",
        "        else:\n",
        "            hidden = self.dropout(hidden[-1,:,:])\n",
        "                \n",
        "        #hidden = [batch size, hid dim]\n",
        "        \n",
        "        output = self.out(hidden)\n",
        "           \n",
        "        return output\n",
        "            "
      ],
      "metadata": {
        "id": "cFMYuD_2C3Vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class BERTLSTMSentiment(nn.Module):\n",
        "      def __init__(self,\n",
        "                  bert,\n",
        "                  hidden_dim,\n",
        "                  output_dim,\n",
        "                  n_layers,\n",
        "                  bidirectional,\n",
        "                  dropout,\n",
        "                  batch_size):\n",
        "          \n",
        "          super().__init__()\n",
        "\n",
        "          self.hidden_dim = hidden_dim\n",
        "          self.bert = bert\n",
        "          \n",
        "          embedding_dim = bert.config.to_dict()['hidden_size']\n",
        "          \n",
        "          self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
        "          \n",
        "          self.label = nn.Linear(hidden_dim, output_dim)\n",
        "          \n",
        "          self.dropout = nn.Dropout(dropout)\n",
        "          self.batch_size= batch_size\n",
        "          \n",
        "\t\n",
        "      def forward(self, input_sentence, batch_size=None):\n",
        "\n",
        "        with torch.no_grad():\n",
        "          embedded = self.bert(input_sentence)[0]\n",
        "\n",
        "        input = embedded.permute(1, 0, 2)\n",
        "\n",
        "     \n",
        "        output, (final_hidden_state, final_cell_state) = self.lstm(input)\n",
        "        final_output = self.label(final_hidden_state[-1]) # final_hidden_state.size() = (1, batch_size, hidden_size) & final_output.size() = (batch_size, output_size)\n",
        "        \n",
        "        return final_output"
      ],
      "metadata": {
        "id": "l0PK1Z3WDB3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HIDDEN_DIM = 768\n",
        "OUTPUT_DIM = 3\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = False\n",
        "DROPOUT = 0.25\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "model = BERTLSTMSentiment(bert,\n",
        "                         HIDDEN_DIM,\n",
        "                         OUTPUT_DIM,\n",
        "                         N_LAYERS,\n",
        "                         BIDIRECTIONAL,\n",
        "                         DROPOUT,\n",
        "                         BATCH_SIZE)\n"
      ],
      "metadata": {
        "id": "Niq7_qblDDWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-summary\n",
        "\n",
        "from torchsummary import summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ab1dXl-_evun",
        "outputId": "bc9b25be-3a9d-4c7c-9f16-0bf35c4f8692"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch-summary\n",
            "  Downloading torch_summary-1.4.5-py3-none-any.whl (16 kB)\n",
            "Installing collected packages: torch-summary\n",
            "Successfully installed torch-summary-1.4.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print network summary\n",
        "summary(model,input_size=(768,),depth=3,batch_dim=1, dtypes=['torch.IntTensor'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vO2s6eMPezmQ",
        "outputId": "582c272b-97ec-441a-82b5-4624fe92ee27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=================================================================\n",
            "Layer (type:depth-idx)                   Param #\n",
            "=================================================================\n",
            "├─BertModel: 1-1                         --\n",
            "|    └─BertEmbeddings: 2-1               --\n",
            "|    |    └─Embedding: 3-1               (23,440,896)\n",
            "|    |    └─Embedding: 3-2               (393,216)\n",
            "|    |    └─Embedding: 3-3               (1,536)\n",
            "|    |    └─LayerNorm: 3-4               (1,536)\n",
            "|    |    └─Dropout: 3-5                 --\n",
            "|    └─BertEncoder: 2-2                  --\n",
            "|    |    └─ModuleList: 3-6              (85,054,464)\n",
            "|    └─BertPooler: 2-3                   --\n",
            "|    |    └─Linear: 3-7                  (590,592)\n",
            "|    |    └─Tanh: 3-8                    --\n",
            "├─LSTM: 1-2                              4,724,736\n",
            "├─Linear: 1-3                            2,307\n",
            "├─Dropout: 1-4                           --\n",
            "=================================================================\n",
            "Total params: 114,209,283\n",
            "Trainable params: 4,727,043\n",
            "Non-trainable params: 109,482,240\n",
            "=================================================================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "=================================================================\n",
              "Layer (type:depth-idx)                   Param #\n",
              "=================================================================\n",
              "├─BertModel: 1-1                         --\n",
              "|    └─BertEmbeddings: 2-1               --\n",
              "|    |    └─Embedding: 3-1               (23,440,896)\n",
              "|    |    └─Embedding: 3-2               (393,216)\n",
              "|    |    └─Embedding: 3-3               (1,536)\n",
              "|    |    └─LayerNorm: 3-4               (1,536)\n",
              "|    |    └─Dropout: 3-5                 --\n",
              "|    └─BertEncoder: 2-2                  --\n",
              "|    |    └─ModuleList: 3-6              (85,054,464)\n",
              "|    └─BertPooler: 2-3                   --\n",
              "|    |    └─Linear: 3-7                  (590,592)\n",
              "|    |    └─Tanh: 3-8                    --\n",
              "├─LSTM: 1-2                              4,724,736\n",
              "├─Linear: 1-3                            2,307\n",
              "├─Dropout: 1-4                           --\n",
              "=================================================================\n",
              "Total params: 114,209,283\n",
              "Trainable params: 4,727,043\n",
              "Non-trainable params: 109,482,240\n",
              "================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 441
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V752uaWaUnZE",
        "outputId": "237c6911-8210-475f-a2e1-bb732d755dd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BERTLSTMSentiment(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (lstm): LSTM(768, 768)\n",
              "  (label): Linear(in_features=768, out_features=3, bias=True)\n",
              "  (dropout): Dropout(p=0.25, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 396
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZU_q6OUoDkhr",
        "outputId": "2de9de3a-54e2-4906-8741-6f2e6d8d9387"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 4,727,043 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():                \n",
        "    if name.startswith('bert'):\n",
        "        param.requires_grad = False"
      ],
      "metadata": {
        "id": "HRWHer9IDo6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiLHRDkMDrZk",
        "outputId": "45fc5cc6-5b9a-47d2-a711-963c5f7072d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 4,727,043 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():                \n",
        "    if param.requires_grad:\n",
        "        print(name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHxBJ0TWDs5p",
        "outputId": "6b299235-fd1a-4e30-892a-9d79c55a746a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lstm.weight_ih_l0\n",
            "lstm.weight_hh_l0\n",
            "lstm.bias_ih_l0\n",
            "lstm.bias_hh_l0\n",
            "label.weight\n",
            "label.bias\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.BCEWithLogitsLoss()"
      ],
      "metadata": {
        "id": "-DCzoEcUDuhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check whether cuda is available\n",
        "if torch.cuda.is_available():    \n",
        "    # If a GPU is available tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "    # Print that a GPU is available and its name\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "# If a GPU is not available print the following statement\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGS380u0Dv6Y",
        "outputId": "d1abe377-13fd-433a-b995-2b5f6c288cfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "metadata": {
        "id": "vlZymgqsEwAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_accuracy(preds, y):\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float() #convert into float for division \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ],
      "metadata": {
        "id": "WjmPs79RE69c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        predictions = model(batch.text).squeeze(1)\n",
        "        \n",
        "        loss = criterion(predictions, batch.label)\n",
        "        \n",
        "        acc = binary_accuracy(predictions, batch.label)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "metadata": {
        "id": "61ELsWOFE9Ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            predictions = model(batch.text).squeeze(1)\n",
        "            \n",
        "            loss = criterion(predictions, batch.label)\n",
        "            \n",
        "            acc = binary_accuracy(predictions, batch.label)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "metadata": {
        "id": "yiKIlN0bE_u3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "metadata": {
        "id": "vMVlff-nFBQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCHS = 10\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "        \n",
        "    end_time = time.time()\n",
        "        \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "        \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'LSTM.PT')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "metadata": {
        "id": "Ou1RFggVFDD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f1_loss(y_pred:torch.Tensor, y_true:torch.Tensor, is_training=False):\n",
        "    '''Calculate F1 score. Can work with gpu tensors'''\n",
        "   \n",
        "    assert y_true.ndim == 1\n",
        "    assert y_pred.ndim == 1 or y_pred.ndim == 2\n",
        "    \n",
        "    if y_pred.ndim == 2:\n",
        "        y_pred = y_pred.argmax(dim=1)\n",
        "    \n",
        "    y_pred = torch.round(torch.sigmoid(y_pred))\n",
        "   \n",
        "   \n",
        "\n",
        "    \n",
        "    tp = (y_true * y_pred).sum().to(torch.float32)\n",
        "    tn = ((1 - y_true) * (1 - y_pred)).sum().to(torch.float32)\n",
        "    fp = ((1 - y_true) * y_pred).sum().to(torch.float32)\n",
        "    fn = (y_true * (1 - y_pred)).sum().to(torch.float32)\n",
        "    \n",
        "    epsilon = 1e-7\n",
        "    \n",
        "    precision = tp / (tp + fp + epsilon)\n",
        "    recall = tp / (tp + fn + epsilon)\n",
        "    \n",
        "    f1 = 2* (precision*recall) / (precision + recall + epsilon)\n",
        "    f1.requires_grad = is_training\n",
        "    return f1, precision, recall, tp, tn, fp, fn\n"
      ],
      "metadata": {
        "id": "mIHcZEeeUQ3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UQGjGuOFY8hM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGbcnyyGecdt",
        "outputId": "98dc0e94-fa60-48ac-93c3-79e6c054c938"
      },
      "source": [
        "N_EPOCHS = 20\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "        \n",
        "    end_time = time.time()\n",
        "        \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "        \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'LSTM.PT')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Epoch Time: 1m 12s\n",
            "\tTrain Loss: 0.261 | Train Acc: 90.53%\n",
            "\t Val. Loss: 0.222 |  Val. Acc: 91.84%\n",
            "Epoch: 02 | Epoch Time: 1m 11s\n",
            "\tTrain Loss: 0.211 | Train Acc: 92.02%\n",
            "\t Val. Loss: 0.206 |  Val. Acc: 92.53%\n",
            "Epoch: 03 | Epoch Time: 1m 11s\n",
            "\tTrain Loss: 0.207 | Train Acc: 92.21%\n",
            "\t Val. Loss: 0.211 |  Val. Acc: 91.92%\n",
            "Epoch: 04 | Epoch Time: 1m 11s\n",
            "\tTrain Loss: 0.195 | Train Acc: 92.62%\n",
            "\t Val. Loss: 0.204 |  Val. Acc: 92.89%\n",
            "Epoch: 05 | Epoch Time: 1m 11s\n",
            "\tTrain Loss: 0.186 | Train Acc: 92.86%\n",
            "\t Val. Loss: 0.195 |  Val. Acc: 92.98%\n",
            "Epoch: 06 | Epoch Time: 1m 11s\n",
            "\tTrain Loss: 0.176 | Train Acc: 93.28%\n",
            "\t Val. Loss: 0.196 |  Val. Acc: 93.00%\n",
            "Epoch: 07 | Epoch Time: 1m 11s\n",
            "\tTrain Loss: 0.166 | Train Acc: 93.39%\n",
            "\t Val. Loss: 0.192 |  Val. Acc: 92.89%\n",
            "Epoch: 08 | Epoch Time: 1m 11s\n",
            "\tTrain Loss: 0.157 | Train Acc: 93.85%\n",
            "\t Val. Loss: 0.205 |  Val. Acc: 92.30%\n",
            "Epoch: 09 | Epoch Time: 1m 11s\n",
            "\tTrain Loss: 0.149 | Train Acc: 93.95%\n",
            "\t Val. Loss: 0.200 |  Val. Acc: 92.96%\n",
            "Epoch: 10 | Epoch Time: 1m 11s\n",
            "\tTrain Loss: 0.136 | Train Acc: 94.50%\n",
            "\t Val. Loss: 0.220 |  Val. Acc: 92.33%\n",
            "Epoch: 11 | Epoch Time: 1m 11s\n",
            "\tTrain Loss: 0.123 | Train Acc: 95.07%\n",
            "\t Val. Loss: 0.210 |  Val. Acc: 93.68%\n",
            "Epoch: 12 | Epoch Time: 1m 11s\n",
            "\tTrain Loss: 0.113 | Train Acc: 95.37%\n",
            "\t Val. Loss: 0.212 |  Val. Acc: 92.99%\n",
            "Epoch: 13 | Epoch Time: 1m 10s\n",
            "\tTrain Loss: 0.093 | Train Acc: 96.03%\n",
            "\t Val. Loss: 0.232 |  Val. Acc: 93.08%\n",
            "Epoch: 14 | Epoch Time: 1m 10s\n",
            "\tTrain Loss: 0.082 | Train Acc: 96.52%\n",
            "\t Val. Loss: 0.279 |  Val. Acc: 88.72%\n",
            "Epoch: 15 | Epoch Time: 1m 11s\n",
            "\tTrain Loss: 0.071 | Train Acc: 96.93%\n",
            "\t Val. Loss: 0.268 |  Val. Acc: 91.62%\n",
            "Epoch: 16 | Epoch Time: 1m 11s\n",
            "\tTrain Loss: 0.063 | Train Acc: 97.44%\n",
            "\t Val. Loss: 0.269 |  Val. Acc: 93.11%\n",
            "Epoch: 17 | Epoch Time: 1m 10s\n",
            "\tTrain Loss: 0.049 | Train Acc: 97.88%\n",
            "\t Val. Loss: 0.269 |  Val. Acc: 92.69%\n",
            "Epoch: 18 | Epoch Time: 1m 11s\n",
            "\tTrain Loss: 0.042 | Train Acc: 98.00%\n",
            "\t Val. Loss: 0.285 |  Val. Acc: 92.51%\n",
            "Epoch: 19 | Epoch Time: 1m 11s\n",
            "\tTrain Loss: 0.037 | Train Acc: 98.28%\n",
            "\t Val. Loss: 0.302 |  Val. Acc: 92.09%\n",
            "Epoch: 20 | Epoch Time: 1m 11s\n",
            "\tTrain Loss: 0.027 | Train Acc: 98.57%\n",
            "\t Val. Loss: 0.357 |  Val. Acc: 92.16%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_acc)\n",
        "print(val_acc)\n",
        "\n",
        "new_tensor = torch.tensor(train_acc, device = 'cpu')\n",
        "print(new_tensor)\n",
        "\n",
        "plt.plot(new_tensor, label='train accuracy')\n",
        "plt.plot(val_acc, label='validation accuracy')\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0.5, 1]);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "oXsKzHp6bSoi",
        "outputId": "70342bb3-ffd0-47d5-eb32-f7da4ba3a823"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.8053, 0.8502, 0.8921, 0.9062, 0.9286, 0.9328, 0.9439, 0.9585, 0.9695, 0.965, 0.9707, 0.9737, 0.9803, 0.9852, 0.9844, 0.9944]\n",
            "[0.9184, 0.9253, 0.9192, 0.9289, 0.9298, 0.93, 0.9289, 0.923, 0.9296, 0.9233, 0.9368, 0.9299, 0.9308, 0.9311, 0.9251, 0.9296]\n",
            "tensor([0.8053, 0.8502, 0.8921, 0.9062, 0.9286, 0.9328, 0.9439, 0.9585, 0.9695,\n",
            "        0.9650, 0.9707, 0.9737, 0.9803, 0.9852, 0.9844, 0.9944])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(train_acc, label='train accuracy')\n",
        "plt.plot(val_acc, label='validation accuracy')\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0.5, 1]);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "OdNWt60hiE4l",
        "outputId": "fa7203e0-6e9f-4bd3-f223-aa7247ae3118"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}